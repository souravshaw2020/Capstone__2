{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.svm import LinearSVC\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('IMDB Dataset.csv')\n",
    "dat=data.copy()\n",
    "final=dat.dropna()\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=final.drop(['sentiment'],axis=1)\n",
    "y=final[\"sentiment\"]\n",
    "for i in y.index:\n",
    "  if y[i]==\"positive\":\n",
    "    y[i]=1\n",
    "  elif y[i]==\"negative\":\n",
    "    y[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = dat['review'].loc[1]\n",
    "soup = BeautifulSoup(review, \"html.parser\")\n",
    "review = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_train, dataset_test, train_data_label, test_data_label = train_test_split(dat['review'], dat['sentiment'], test_size=0.25, random_state=42)\n",
    "\n",
    "train_data_label = (train_data_label.replace({'positive': 1, 'negative': 0})).values\n",
    "test_data_label  = (test_data_label.replace({'positive': 1, 'negative': 0})).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = []\n",
    "corpus_test  = []\n",
    "with open('finalReview.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"review\", \"sentiment\"])\n",
    "    for i in range(dataset_train.shape[0]):\n",
    "        soup = BeautifulSoup(dataset_train.iloc[i], \"html.parser\")\n",
    "        review = soup.get_text()\n",
    "        review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "        lem = WordNetLemmatizer()\n",
    "        review = [lem.lemmatize(word) for word in review]\n",
    "        review = ' '.join(review)\n",
    "        writer.writerow([review, train_data_label[i]])\n",
    "        corpus_train.append(review)\n",
    "      \n",
    "    for j in range(dataset_test.shape[0]):\n",
    "        soup = BeautifulSoup(dataset_test.iloc[j], \"html.parser\")\n",
    "        review = soup.get_text()\n",
    "        review = re.sub('\\[[^]]*\\]', ' ', review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        review = review.lower()\n",
    "        review = review.split()\n",
    "        review = [word for word in review if not word in set(stopwords.words('english'))]\n",
    "        lem = WordNetLemmatizer()\n",
    "        review = [lem.lemmatize(word) for word in review]\n",
    "        review = ' '.join(review)\n",
    "        writer.writerow([review, test_data_label[j]])\n",
    "        corpus_test.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "tfidf_vec_train = tfidf_vec.fit_transform(corpus_train)\n",
    "tfidf_vec_test = tfidf_vec.transform(corpus_test)\n",
    "\n",
    "linear_svc = LinearSVC(C=0.5, random_state=42)\n",
    "linear_svc.fit(tfidf_vec_train, train_data_label)\n",
    "\n",
    "predict = linear_svc.predict(tfidf_vec_test)\n",
    "\n",
    "check =pd.read_csv('dataset.csv')\n",
    "\n",
    "Test_Data = check['Review']\n",
    "\n",
    "Checker_Data = tfidf_vec.transform(Test_Data)\n",
    "predict_test = linear_svc.predict(Checker_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'final_model.sav'\n",
    "joblib.dump(linear_svc,filename)\n",
    "joblib.dump(tfidf_vec,filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
